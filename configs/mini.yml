# mini config for quick testing purposes
system:
  device: gpu  # [cpu, gpu, tpu]

chem:
  fp_option:
    type: morgan
    morgan_radius: 2
    morgan_n_bits: 256
  building_block_path: data/building_blocks/Enamine_Rush-Delivery_Building_Blocks-US_223244cmpd_20231001.sdf
  reaction_path: data/rxn_templates/comprehensive.txt
  rxn_matrix: data/processed/comp_256/matrix.pkl
  fpindex: data/processed/comp_256/fpindex.pkl
  # !!
  # TODO: separate into train/val/test sets and put separate paths
  protein_molecule_pairs_path: data/protein_molecule_pairs/papyrus_selection_182129.csv
  protein_embedding_path: data/protein_embeddings/embeddings_selection_float16_4973.pth 
  synthetic_pathways_path: data/synthetic_pathways/filtered_pathways_290000.pth
  # !!

data:
  init_stack_weighted_ratio: 0.90

model:
  # !!
  encoder_type: protein 
  encoder:
    d_model: 1152  # ESM num embedding dimensions
  # !!
  decoder:
    d_model: 1
    nhead: 1
    dim_feedforward: 256
    num_layers: 1
    pe_max_len: 1
    output_norm: false
    fingerprint_dim: ${chem.fp_option.morgan_n_bits}
    num_reaction_classes: 120
  fingerprint_head_type: diffusion_transformer
  fingerprint_head:
    d_model: 1
    fingerprint_dim: ${chem.fp_option.morgan_n_bits}
    diffusion_steps: 1
    diffusion_s: 0.01
    # Transformer should be larger for decoder-only setting
    n_bits_per_word: 1
    nhead: 1
    dim_feedforward: 256
    num_layers: 1
    norm: false
    num_training_fingerprints_per_sample: 1
    enable_gradient_checkpointing: false

train:
  loss_weights:
    token: 1.0
    reaction: 1.0
    fingerprint_diffusion: 0.5
    fingerprint_bce: 0.0  # For monitoring purpose only
  val_loss_weights:
    token: 1.0
    reaction: 1.0
    fingerprint_diffusion: 0.0  # Multinomial diffusion loss does not drop
    fingerprint_bce: 0.5
  max_iters: 1_000_000
  val_freq: 5000
  max_grad_norm: 100.0
  optimizer:
    type: adamw
    lr: 1.e-4
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.999
  scheduler:
    type: plateau
    factor: 0.8
    patience: 10
    min_lr: 1.e-5
